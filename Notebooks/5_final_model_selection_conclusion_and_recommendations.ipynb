{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In notebook **2. Modeling, Hyperparameter Tuning and Model Evaluation** we built and evaluated a variatey of regression models to predict damage costs in dollar values of wildfires in California in a given county. The 3 contexts for model building were the base case, the grid search and the model stacking. Below are visual summaries of the scores for each model in each of these 3 contexts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The Base Case Scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![base_case](../images/base_case_scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Summary\n",
    "The average test score for all of the models in the base case context is 0.41 with an average difference between test and train scores (which we use as a proxy measure of how large the error due to variance is) is 0.32.\n",
    "\n",
    "The next step was to build these same models after thoroguh hyperparameter tuning to see if the overall test scores increase and whether the error due to high variance may decrease. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Grid Search Scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gs_case](../images/grid_search_scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Summary \n",
    "\n",
    "The average test score for all of the models in the grid search context is 0.49 with an average difference between test and train scores of 0.21.\n",
    "We see some increase in the average test scores and a significant decrease in error due to variance after tuning the models for the best hyperparameters. \n",
    "\n",
    "One additionl step if to try stacking the models to see if we can achieve even better performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Meta-Model Scores (Stacking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stack_case](../images/stack_model_scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Summary \n",
    "\n",
    "The average test score for all of the models in the stacked-model context is 0.515 with an average difference between test and train scores  of 0.405.\n",
    "We see some a significant increase in overall test scores, however, the error due to high variance also has increase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Revisiting the Evaluation Criteria "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In notebook **2.Modeling, Hyperparameter Tuning and Model Evaluation** we established a criteria for model performance measure which would help us select a model as the best in performance. The criteria was to build a model which best satisfies the following three conditions: \n",
    "\n",
    "- The highest possible training and testing scores AND\n",
    "- The smallest difference between training and testing scores AND\n",
    "- The smallest different between training score and cross_val score and the smallest difference between testing score and cross_val score.\n",
    "\n",
    "A model that satisfies these evaluation criteria is selected because achieving such scores enables us to more relaibly count on the model having decent predictive power. This boils down to the bias and variance trade-of. A model that has a large difference between train and test score (with train score being much higher than train score) suffers from high variance - such as model is overfit and will not generalize well on out of sample data. \n",
    "\n",
    "Contrary to high variance problem - a model with generally low train and test scores overall (including cross val score) is said to suffer from high bias - such a model is underfit and does not accurately represent the true relationships and patterns in the data. \n",
    "\n",
    "By striving to satisfy the criteria above, we make an attempt to find that sweet spot of miimizing both variance and bias. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Discussion of Our Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the average test score of meta-models are higher than the average test scores of both the base case and the grid searched models, we choose not to select our best model from the meta-models because the meta-models on average suffer from error due to very high variance, therefore they will not hold good generalizability to unseen data. \n",
    "\n",
    "The best set of models to choose from is the set of grid searched models. These models have a relatively high average testing score as compared to the base case models, and significantly reduced variance overall. \n",
    "\n",
    "The best model candidate is therefore the one among the grid searched models which best satisfies the evaluation criterian mentioned above. \n",
    "\n",
    "**The best model** is the grid-searched lasso regularized model with the following scores: \n",
    "- cv score:0.43\n",
    "- train score: 0.50\n",
    "- test score: 0.49\n",
    "\n",
    "The best parameters of this model are  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![best_model](../images/best_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Existing Model Limitations\n",
    "\n",
    "The model we build is far from perfect. \n",
    "\n",
    "**1. Performance Has Room for Improvement:** With a test sore of 0.49 this model can only explain about 49% of the variability in the data. However this is not to say the a better model cannot be built. Understanding the nature of wildfires and how quickly and how much they spread is a very complex problem which depends on complex interactions of a large number of variables (leave alone predicting the cost of damage to a county).\n",
    "\n",
    "**2. Model is based on many assumptions:** From imputing missing values, to assumptions made about what features we think may be predictive for the target variable, our model realies on a lot of assimptions that allowed us to aggregate data in a meainigul ways from a variety of source. \n",
    "\n",
    "**3. The data may not be good:** We know for a fact that our data used here is not complete or granular enough to be reliable. This is mainly due to scarcity of good data and data  that's actualy available being available in many different formats in many different places but not in any way that can be easily combined. Our journey through data collection confirms FEMA's pain on there being a large gap in the data the organization needs in order to efficiently allocate resources which hinders the organizations' achivement of fulfilling its mission in the best way possible. More information on this can be found in this [article](https://www.pewtrusts.org/en/research-and-analysis/reports/2018/06/19/what-we-dont-know-about-state-spending-on-natural-disasters-could-cost-us) published by FEMA. Ideally, we would want data on a fire-by-fire basis, as opposed to the data we have which contains information about fires in general on a county-county-basis. \n",
    "\n",
    "**4. Our model is built on the assumption that there will be a time lag between the wirldfire incident and the time when FEMA will need to step-in:** Generally, the state needs to declare an emergency and the governor of the state must formally request FEMA for assistance within a month of the disaster occurance. FEMA then takes additional time to assess the emergency and to determine whether the emergency qualifies as severe enough for FEMA's assistance to deploy. These steps imply that there will be some time on the order of days and perhaps weeks passed between the start of a wildfire and FEMA taking action. The assumption for this time-lag suggests that there will be historical data available to FEMA on things like acres burned and number of fires and other features which we use in our model. However, the model does not account for immediacy in making decisions at the present. If an emergency is pressing enough and there is no time to wait to acquire all the necessary data, then the model is useless as it does require historical data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. How to Improve the Existing Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the problem we wish to solve is very complex due to the vast number of variables involved and interacting with one another that determine the nature of any given wildfire, and is also difficult to solve due to scarcity in meaningful data, the existing model can still be improved in the following ways:\n",
    "\n",
    "**Approach 1. Add more predictive features to the model** \n",
    "    \n",
    "- the number of structures damaged or destroyed as a result of fires and the acres of land burned: we tested another data set that contained the acres burned and the number of structures destroyed as features with a hypotheses testfor the pearson's correlation coefficient and found that there was a statistically significant correlation between the interaction of these features and the target variable. Unfortunately the dataset we tested on could not be combined with our data due to it being on a state-by-state basis rather than county-county-basis.  \n",
    "- the number of fatalities or injured individuals which may be out of employment for a long time\n",
    "- incorporation of actuarial data (home renter's insurance premiums are calculated based on known calculations of disaster likelihood and expected damage scales associated with them)\n",
    "- a more refined feature which more accurately represents the percentage of wilderness vs developed area in a given county\n",
    "\n",
    "**Approach 2. Conduct more in-depth analyses to aid in feature selection**\n",
    "\n",
    "We used our collective knowlegde, common sense and some research to select and engineer features which we thought maybe predictive of the target variable. However, the model may be further improved by \n",
    "- employing more accurate and reliable domain knowledge from an expert who better understands the nature of wildfires\n",
    "- employing automated techniques for feature selection such as decision trees and feature importances assigned to feather by them, PCA if the dimensionality of an expanded dataset with added features becomes very large, polynomial expansion of the data to try out more interaction terms which may be important, etc...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. How to Expand the Existing Model\n",
    "\n",
    "In section 6 of this notebook, we mentioned a fourth limitation of the model. The model build herein uses historical data to make predictions. However, sometimes it's necesary to have a model that uses live-streaming data to make predictions on a currently evolving situation. Our model here can be expanded into a more complex model that may be able to account for such a 'present-time' pressing demand for predictions. We suggest here to combine this model with antoher model that is designed to employ live-news/tweets and other social media to inform the current statust-quo and update data on much more frequent basis than can be done with historical data on a days/weeks basis. Such a model would utilize advanced natural language processing techniques to analyse text data from constantly reporting news from which necessary data can be extracted to make projections on a more immediate and present basis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our recommendations to FEMA are as follows: \n",
    "    \n",
    "- If possible, create guidlines or create a standardized process for states/counties and local governments to collect data on wildfires on a more granular level to bridge the massive gap in the data that makes predictions of damage cost forecasting so dfficult. \n",
    "\n",
    "- If possible mandate and enforce these data reporting/collection guidlines to ensure that states/counties and local governments are actually fulfilling these requirements. \n",
    "\n",
    "- If possible, such data should be entered into a standardized/centralized data base for easy and quick access as a large amount of time and a massive amount of human effort may be neceesary to collect and combined the data. \n",
    "\n",
    "- Wildfire and more generally natural disaster data collection should be enforced by law because it can help save lives of individuals and families, and it can help reduce an overall economic impact to the nation's whole economy if assistance, mitigation of disasters, management of disaster supression where possible and post-disaster resource allocation can be done more efficiently and quickly thanks to in-advance projections of damages before they happen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
